### 面试总结

> 技术部分

* SVM和LogisticRegression的区别：
  1. SVM的损失函数可以是hinge loss，LR的损失函数最好是cross entropy。换言之，SVM只需要考虑构成分类面的实例点，而LR需要考虑所有点且相对于泛用的squared error，可以单独设计针对离散值0，1设计log error，合并即cross entropy error。
  2. 用概率解释逻辑回归的输出，假设模型IID，输出结果为0，1。因此使用伯努利分布，p(1) = h(x), p(0) = 1 - h(x), p(y|x,w) = (h(x)^y) * ((1 - h(x))^(1 - y)。使用MLE即可得loss function。（同理，对于linear regression，用高斯分布即可获得least squared loss function）
  3. SVM依赖于数据，因此对于较大差异特征需要normalization，以避免不同维度特征测度不同。而基于概率的LR只有在需要加入正则化项时需要normalization。但是对于使用gradient descent算法调优的模型，都需要feature scaling。
  4. SVM是结构风险最小化，LR是经验风险最小化。因此SVM不需要正则项而LR需要以避免overfitting。
  5. LR也是可以使用核函数的，但是每点两两计算核函数太过复杂，而SVM通过对偶问题转化o(dn^2)相对轻松。
  6. 对于小规模数据SVM，大规模LR训练简单，更多被采用。
  * 
  
* 说说分类模型中的SVM，PDTree和logistic Regression区别，PDTree没接触过，logistic没说到核函数，总的来说没答到精髓？
* 说说图像用的最熟的模型，MLP+PCA.然后就问CNN模型，我回答RCNN和VGG，后面就问他们二者和一般CNN的区别。
* 说一片你熟悉的论文，我说的是cycleGAN,然后让我讲解原理，评价说我只讲了一般GAN的原理，为什么CYCLE没说出来
* 说一下你为什么出国，出国学到了什么，为什么回来，没怎么准备，答的比较差。
* 问有没有什么大项目，我说暂时没，正在和导师做汽车识别。问做了什么，标记和合成分类图像。
